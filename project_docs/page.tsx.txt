"use client";

import { useState, useEffect, useCallback, useRef } from 'react';
import { useMic, RecordingStatus } from './hooks/useMic';
import { PromptType } from '@/lib/interviewPrompts';
import { SINGLE_PROMPT } from '@/lib/singlePrompt';
import { RubricResult } from '@/lib/rubricPrompts';
import { Mic } from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import OpenAI from 'openai'; // Import OpenAI for ChatCompletionMessageParam type

// Define the states for our application flow
type AppState =
  | "idle"          // Ready to start or waiting for next turn
  | "recording"     // User is speaking
  | "transcribing"  // Sending audio, waiting for transcript
  | "summarizing"   // NEW: Summarizing chat history (state added for UI feedback)
  | "asking"        // Sending turn info, waiting for AI question
  | "speaking"      // AI question received, sending for TTS, waiting for audio
  | "playing"       // AI audio is playing
  | "finished"      // Interview complete, ready to process feedback
  | "processing_feedback" // Calling pipeline API
  | "show_results"  // Displaying feedback
  | "error";        // An error occurred

type ChatMessage = {
  role: "user" | "examiner"; // Roles for displaying to user in fullChatHistory
  content: string;
};

// NEW CONSTANTS: Define how many user turns to keep in the 'live' buffer and the summarization interval.
// MESSAGES_TO_KEEP_USER_TURNS: The number of user turns (and corresponding assistant turns)
// to retain in the llmContextMessages buffer after summarization.
const MESSAGES_TO_KEEP_USER_TURNS = 3;
// SUMMARIZATION_INTERVAL_USER_TURNS: Defines how often (in user turns) to trigger a summarization.
// E.g., if 4, a summary will be triggered after user's 4th, 8th, 12th response, etc.
const SUMMARIZATION_INTERVAL_USER_TURNS = 4;


export default function Page() {
  const [isMounted, setIsMounted] = useState(false);
  const [appState, setAppState] = useState<AppState>("idle");

  // fullChatHistory for display and final pipeline evaluation. This is the complete, untouched record.
  const [fullChatHistory, setFullChatHistory] = useState<ChatMessage[]>([]);
  const fullChatHistoryRef = useRef(fullChatHistory); // Ref for stable access in useEffects
  useEffect(() => {
    fullChatHistoryRef.current = fullChatHistory;
  }, [fullChatHistory]);

  // llmContextMessages for sending to the LLM. This will be summarized.
  // It uses OpenAI's message format directly as it's the payload for the LLM API.
  const [llmContextMessages, setLlmContextMessages] = useState<OpenAI.Chat.ChatCompletionMessageParam[]>([]);
  const llmContextMessagesRef = useRef(llmContextMessages); // Ref for stable access in useEffects
  useEffect(() => {
    llmContextMessagesRef.current = llmContextMessages;
    console.log("Page.tsx Debug: LLM Context Messages updated:", JSON.stringify(llmContextMessages, null, 2)); // Debug log: show full LLM context
  }, [llmContextMessages]);

  // Counter for user turns to trigger summarization
  // Resets to 0 after summarization.
  const [userTurnCounter, setUserTurnCounter] = useState(0);

  const [lastError, setLastError] = useState<string | null>(null);
  const [feedbackResults, setFeedbackResults] = useState<RubricResult[]>([]);
  const [isChatVisible, setIsChatVisible] = useState(false);
  const audioPlayerRef = useRef<HTMLAudioElement>(null);

  const {
    status: micStatus,
    audioBlob,
    startRecording,
    stopRecording,
    reset: resetMic,
    error: micError,
    isAudioApiSupported
  } = useMic();

  // Set mounted state only on client, primarily for hydration
  useEffect(() => {
    console.log("Page.tsx Debug: Component mounted, setting isMounted to true.");
    setIsMounted(true); // Ensure this is set for rendering the main UI
  }, []);

  // === API Call Functions ===

  const callTranscribeApi = useCallback(async (audioData: Blob) => {
    setAppState("transcribing");
    console.log("Page.tsx Debug: callTranscribeApi initiated.");
    try {
      const formData = new FormData();
      formData.append('audio', audioData, 'audio.webm');

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || `Transcription API failed with status ${response.status}`);
      }

      const data = await response.json();
      const transcript = data.transcript;

      if (transcript) {
        console.log(`Page.tsx Debug: Transcription successful: "${transcript}"`);
        // Add user message to full history for display/pipeline
        setFullChatHistory(prev => [...prev, { role: 'user', content: transcript }]);

        // Add user message to LLM context history (as 'user' role)
        setLlmContextMessages(prev => [...prev, { role: 'user', content: transcript }]);

        // Increment user turn counter ONLY after a successful user transcript
        setUserTurnCounter(prev => prev + 1);

        // Transition to 'asking'. The summarization useEffect will intercept if needed.
        setAppState("asking");

      } else {
        throw new Error("Transcription API returned empty transcript.");
      }

    } catch (err) {
      console.error("Page.tsx Error: Transcription error:", err);
      setLastError(err instanceof Error ? err.message : "Transcription failed");
      setAppState("error");
    }
  }, []);

  // NEW: Summarization API call function
  // Responsibility: Send conversational messages to the /api/summarize endpoint and update llmContextMessages with the summary.
  const callSummarizeApi = useCallback(async (messagesToSummarize: OpenAI.Chat.ChatCompletionMessageParam[], messagesToKeep: OpenAI.Chat.ChatCompletionMessageParam[]) => {
    setAppState("summarizing"); // Set app state to show loading/processing
    console.log("Page.tsx Debug: Initiating summarization API call...");
    console.log("Page.tsx Debug: Messages being sent for summarization:", JSON.stringify(messagesToSummarize, null, 2)); // Debug log
    try {
      const response = await fetch('/api/summarize', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ messages: messagesToSummarize }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || `Summarize API failed with status ${response.status}`);
      }

      const data = await response.json();
      const summary = data.summary;

      if (summary) {
        console.log("Page.tsx Debug: Summary received:", summary); // Debug log

        // CORRECTED LOGIC FOR SUMMARY AS ASSISTANT MESSAGE:
        // 1. Start new context with ONLY the main system prompt (therapy bot persona)
        let newContext: OpenAI.Chat.ChatCompletionMessageParam[] = [
          { role: "system", content: SINGLE_PROMPT.prompt_text }
        ];

        // 2. Add the summary as an 'assistant' role message.
        // This is a special internal assistant message for context, not spoken to the user.
        newContext.push({
          role: "assistant",
          content: `I've summarized our previous exchanges for context: ${summary}`
        });

        // 3. Append the messages that were designated to be kept (the recent buffer)
        newContext = newContext.concat(messagesToKeep);

        setLlmContextMessages(newContext); // Update the LLM context history
        setUserTurnCounter(0); // Reset the counter after successful summarization

        // Return to idle after summarization to allow user to speak next
        setAppState("idle");
      } else {
        throw new Error("Summarize API returned empty summary.");
      }

    } catch (err) {
      console.error("Page.tsx Error: Summarization error:", err);
      setLastError(err instanceof Error ? err.message : "Summarization failed");
      setAppState("error");
    }
  }, []); // Dependencies for useCallback: None, as it uses arguments for messages

  // Responsibility: Send the LLM context to the /api/ask endpoint to get the next question.
  const callAskApi = useCallback(async (promptToSend: PromptType, currentLlmContextMessages: OpenAI.Chat.ChatCompletionMessageParam[]) => {
    if (!promptToSend) {
      setLastError("Error: No active prompt found for this turn.");
      setAppState("error");
      return;
    }
    setAppState("asking"); // Keep in asking state while waiting for response
    console.log("Page.tsx Debug: Initiating Ask API call...");
    console.log("Page.tsx Debug: Sending LLM Context Messages to Ask API:", JSON.stringify(currentLlmContextMessages, null, 2)); // Debug log
    try {
      const response = await fetch('/api/ask', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        // IMPORTANT: Send llmContextMessages, which includes the rolling summary
        body: JSON.stringify({ prompt: promptToSend, history: currentLlmContextMessages }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || `Ask API failed with status ${response.status}`);
      }

      const data = await response.json();
      const question = data.question;

      if (question) {
        console.log(`Page.tsx Debug: Question received: "${question}"`);
        // Add examiner message to full history for display/pipeline
        setFullChatHistory(prev => [...prev, { role: 'examiner', content: question }]);
        // Add examiner message to LLM context history (as 'assistant' role for LLM)
        setLlmContextMessages(prev => [...prev, { role: 'assistant', content: question }]);
        setAppState("speaking"); // Move to speaking state
      } else {
        throw new Error("Ask API returned empty question.");
      }

    } catch (err) {
      console.error("Page.tsx Error: Ask API error:", err);
      setLastError(err instanceof Error ? err.message : "Failed to get question");
      setAppState("error");
    }
  }, []); // Dependencies: promptToSend (SINGLE_PROMPT), llmContextMessagesRef

  // Responsibility: Send text to the /api/speak endpoint to get audio for playback.
  const callSpeakApi = useCallback(async (text: string) => {
    setAppState("speaking");
    console.log("Page.tsx Debug: Initiating Speak API call for text:", text); // Debug log
    try {
      const response = await fetch('/api/speak', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: text }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || `Speak API failed with status ${response.status}`);
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);

      const player = audioPlayerRef.current;
      if (player) {
        player.src = audioUrl;
        player.load();
        player.play()
          .then(() => {
            console.log("Page.tsx Debug: Audio playback started successfully.");
            setAppState("playing");
          })
          .catch(playError => {
            console.error("Page.tsx Error: Audio playback error:", playError);
            setLastError("Failed to play examiner audio.");
            setAppState("error");
            URL.revokeObjectURL(audioUrl); // Clean up URL if play fails
          });
      } else {
         throw new Error("Audio player reference not found.");
      }

    } catch (err) {
      console.error("Page.tsx Error: Speak API error:", err);
      setLastError(err instanceof Error ? err.message : "Failed to generate speech");
      setAppState("error");
    }
  }, []); // No external dependencies

  // Responsibility: Send the full user transcript to the /api/pipeline endpoint for final evaluation.
  const callPipelineApi = useCallback(async () => {
    setAppState("processing_feedback");
    console.log("Page.tsx Debug: Initiating Pipeline API call for final feedback."); // Debug log
    try {
      // Concatenate user responses from FULL chat history for comprehensive analysis
      const fullTranscript = fullChatHistoryRef.current
        .filter(msg => msg.role === 'user')
        .map(msg => msg.content)
        .join('\n\n'); // Separate turns with double newline

      if (!fullTranscript) {
        throw new Error("Cannot process feedback: No user responses found in history.");
      }
      console.log("Page.tsx Debug: Full transcript for pipeline:", fullTranscript); // Debug log

      const response = await fetch('/api/pipeline', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ transcript: fullTranscript }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || `Pipeline API failed with status ${response.status}`);
      }

      const data = await response.json();
      const feedback = data.feedback;

      if (feedback && Array.isArray(feedback)) {
        setFeedbackResults(feedback);
        setAppState("show_results");
      } else {
        throw new Error("Pipeline API returned invalid feedback format.");
      }

    } catch (err) {
      console.error("Page.tsx Error: Pipeline API error:", err);
      setLastError(err instanceof Error ? err.message : "Failed to process feedback");
      setAppState("error");
    }
  }, [fullChatHistoryRef]); // Depends on fullChatHistoryRef for the complete transcript

  // === Effect Hooks for State Transitions ===

  // Effect to handle mic status changes: triggers transcription.
  useEffect(() => {
    console.log(`Page.tsx Debug: micStatus changed to ${micStatus}, audioBlob is ${audioBlob ? 'present' : 'absent'}.`);
    if (micStatus === 'stopped' && audioBlob) {
      console.log("Page.tsx Debug: Mic status stopped, audioBlob available. Triggering transcription.");
      callTranscribeApi(audioBlob);
    } else if (micStatus === 'error' && micError) {
      console.error("Page.tsx Error: Mic error detected:", micError.message);
      setLastError(`Microphone Error: ${micError.message}`);
      setAppState('error');
    }
  }, [micStatus, audioBlob, micError, callTranscribeApi]);

  // NEW/MODIFIED Effect to trigger summarization based on userTurnCounter and appState.
  // Responsibility: Decide when to call the summarization API based on conversational turns and timing.
  useEffect(() => {
    // EXTRA DEBUG: Log the values at the start of this effect for precise timing
    console.log(`Page.tsx Debug: Summarization trigger effect evaluated. userTurnCounter: ${userTurnCounter}, appState: ${appState}.`);

    // Trigger summarization if:
    // 1. userTurnCounter is a multiple of SUMMARIZATION_INTERVAL_USER_TURNS (e.g., 4, 8, 12...)
    // 2. userTurnCounter is greater than 0 (don't summarize before any turns)
    // 3. appState is 'idle' (meaning AI has finished speaking and user is about to speak or waiting for prompt)
    if (userTurnCounter > 0 && userTurnCounter % SUMMARIZATION_INTERVAL_USER_TURNS === 0 && appState === 'idle') {
      console.log(`Page.tsx Debug: Summarization criteria met. User turn count is ${userTurnCounter}. App state is 'idle'. Initiating summarization process.`);

      // Get the current LLM context messages from the ref.
      const currentLlmMessages = llmContextMessagesRef.current;

      // Filter out the initial SINGLE_PROMPT system message (always at index 0)
      // and any previous summary assistant messages (starting with "I've summarized...")
      // to get only the *actual conversational* messages for summarization/retention.
      const conversationalMessagesOnly = currentLlmMessages.filter(msg =>
        msg.role === 'user' || (msg.role === 'assistant' && typeof msg.content === 'string' && !msg.content.startsWith("I've summarized our previous exchanges for context:"))
      );

      // Calculate how many conversational messages to keep in the buffer (3 user + 3 assistant = 6 messages)
      const messagesToKeepCount = MESSAGES_TO_KEEP_USER_TURNS * 2; // Total pair messages (user + assistant)

      // If there are more pure conversational messages than we want to keep in the buffer,
      // then we have older messages that need to be summarized.
      if (conversationalMessagesOnly.length > messagesToKeepCount) {
        // Messages to summarize are the older ones, before the recent buffer.
        const messagesToSummarize = conversationalMessagesOnly.slice(0, conversationalMessagesOnly.length - messagesToKeepCount);
        // Messages to keep are the most recent ones (the buffer).
        const messagesToKeep = conversationalMessagesOnly.slice(conversationalMessagesOnly.length - messagesToKeepCount);

        console.log(`Page.tsx Debug: Will summarize ${messagesToSummarize.length} messages and keep ${messagesToKeep.length} messages for the next LLM context.`);
        callSummarizeApi(messagesToSummarize, messagesToKeep);
      } else {
        console.log(`Page.tsx Debug: Not enough conversational messages (${conversationalMessagesOnly.length}) to trigger summarization (threshold: ${messagesToKeepCount}). Proceeding normally.`);
        setAppState("idle"); // Ensure we're idle if no summarization happens here
      }
    }
  }, [userTurnCounter, appState, callSummarizeApi]); // Dependencies ensure this effect re-evaluates when relevant state changes

  // Effect to handle app state changes for API calls (ask, speak, pipeline).
  // Responsibility: Control the sequence of API calls based on the current application state.
  useEffect(() => {
    console.log(`Page.tsx Debug: App state effect triggered. Current appState: ${appState}`);
    if (appState === 'asking') {
      console.log("Page.tsx Debug: App state is 'asking'. Calling Ask API.");
      // Send the current llmContextMessages (which will include the initial prompt and any summary)
      callAskApi(SINGLE_PROMPT, llmContextMessagesRef.current);
    } else if (appState === 'speaking') {
      console.log("Page.tsx Debug: App state is 'speaking'. Preparing examiner audio.");
      // Get the last examiner message from the full history for speech synthesis
      const lastExaminerMessage = fullChatHistoryRef.current.findLast(msg => msg.role === 'examiner');
      if (lastExaminerMessage) {
        callSpeakApi(lastExaminerMessage.content);
      } else {
        console.error("Page.tsx Error: Cannot speak: No examiner question found in full history.");
        setLastError("Cannot speak: No examiner question found.");
        setAppState("error");
      }
    } else if (appState === 'finished') {
        console.log("Page.tsx Debug: App state is 'finished'. Triggering pipeline for feedback.");
        callPipelineApi();
    }
  }, [appState, callAskApi, callSpeakApi, callPipelineApi]);

  // Effect to handle audio playback ending: transitions to idle for next user input.
  useEffect(() => {
    const player = audioPlayerRef.current;

    const handleAudioEnd = () => {
      console.log("Page.tsx Debug: 'ended' event fired for audio player. Returning to idle for next turn.");
      setAppState("idle");
    };

    if (player) {
      console.log("Page.tsx Debug: Binding 'ended' event listener to audio player.");
      player.addEventListener('ended', handleAudioEnd);

      // Cleanup function to remove event listener
      return () => {
        console.log("Page.tsx Debug: Cleaning up 'ended' event listener.");
        player.removeEventListener('ended', handleAudioEnd);
      };
    }
  }, [appState]); // Dependency: appState (to re-bind if player changes or state resets)

  const handleResetSession = useCallback(() => {
    console.log("Page.tsx Debug: Reset Session button clicked. Resetting state.");

    // Stop any media
    if (appState === 'recording') {
      stopRecording();
    }
    if (audioPlayerRef.current) {
      audioPlayerRef.current.pause();
      audioPlayerRef.current.src = "";
    }

    // Reset all state
    setAppState("idle");
    setFullChatHistory([]);
    setLlmContextMessages([{ role: "system", content: SINGLE_PROMPT.prompt_text }]);
    setUserTurnCounter(0);
    setFeedbackResults([]);
    setLastError(null);
    resetMic();
  }, [appState, stopRecording, resetMic]);

  // Handler for microphone button press.
  // Responsibility: Control the start/stop of recording and manage interview resets.
  const handleMicPress = useCallback(() => {
    console.log(`Page.tsx Debug: Mic button pressed. Current appState: ${appState}`);
    if (appState === "recording") {
      console.log("Page.tsx Debug: Mic button pressed while recording. Stopping recording.");
      setAppState("transcribing"); // Immediately transition to transcribing state for UI feedback
      stopRecording();
    }
    // If idle, error, or showing results, the action is to start recording or restart interview.
    else if (appState === "idle" || appState === 'error' || appState === 'show_results') { // Ensure appState 'error' also allows new start

      // Determine if this is the start of a brand-new interview or just continuing.
      const isNewInterview = fullChatHistoryRef.current.length === 0 || appState === 'error' || appState === 'show_results';

      if (isNewInterview) {
        console.log(`Page.tsx Debug: Starting new interview. Resetting all state variables.`);
        setFullChatHistory([]); // Reset full display history
        // Initialize llmContextMessages with ONLY the system prompt for the therapy bot's persona.
        setLlmContextMessages([{ role: "system", content: SINGLE_PROMPT.prompt_text }]);
        setUserTurnCounter(0);    // Reset user turn counter
        setFeedbackResults([]);   // Clear any old feedback
        setLastError(null);       // Clear any old errors
      } else {
        console.log("Page.tsx Debug: Continuing existing interview. Preparing for next recording.");
      }

      resetMic(); // Reset microphone hook state
      setLastError(null); // Clear any pre-existing errors

      setAppState("recording"); // Transition to recording state
      startRecording();         // Start microphone recording
    }
  }, [appState, fullChatHistoryRef, resetMic, startRecording, stopRecording]);

  // Toggle chat visibility handler
  const toggleChatVisibility = useCallback(() => {
    setIsChatVisible(prev => !prev);
    console.log(`Page.tsx Debug: Toggling chat visibility to ${!isChatVisible}`);
  }, [isChatVisible]); // Dependency: isChatVisible

  // Render a basic loading or placeholder state until mounted
  if (!isMounted) {
    console.log("Page.tsx Debug: Component not yet mounted, rendering loading state.");
    return (
       <div className="grow flex flex-col p-4 space-y-4 max-w-3xl mx-auto items-center justify-center">
         <h1 className="text-2xl font-bold">IELTS Speaking Mock Test v7 (Therapy Bot Adaptation)</h1>
         <p>Loading application...</p>
       </div>
    );
  }

  return (
    <>
      {/* Status Display - now outside the centered container */}
      <div className="w-full text-sm text-gray-500 text-center whitespace-nowrap py-2 bg-gray-50 border-b border-gray-200">
        Status: <span className="font-semibold">{appState}</span> | Mic Status: <span className="font-semibold">{micStatus}</span> | User Turns: <span className="font-semibold">{userTurnCounter}</span>
        {!isAudioApiSupported && <p className="text-red-500 font-bold">Error: Audio Recording NOT Supported in this browser!</p>}
        {micError && <p className="text-red-500 font-bold">Mic Error: {micError.message}</p>}
        {lastError && <p className="text-red-500 font-bold">App Error: {lastError}</p>}
      </div>
      <div className="grow flex flex-col p-4 space-y-4 max-w-3xl mx-auto">
        <h1 className="text-2xl font-bold text-center">IELTS Speaking Mock Test (Therapy Bot Adaptation)</h1>

        {/* Loading spinner for processing feedback or summarizing */}
        {appState === 'processing_feedback' || appState === 'summarizing' ? (
          <div className="flex flex-col items-center justify-center h-64 border rounded p-4 bg-white shadow-sm">
            <div className="mb-4">
              <span className="inline-block w-12 h-12 border-4 border-gray-300 border-t-blue-500 rounded-full animate-spin" />
            </div>
            <p className="text-center text-gray-700 text-lg font-medium">
                {appState === 'processing_feedback' ? "I'm marking your score and generating feedback, this may take a minute." : "Summarizing our conversation to remember key points..."}
            </p>
          </div>
        ) : (
        <>
        {/* Control Buttons */}
        <div className="flex space-x-2 mb-4 justify-center">
          {/* Button to toggle chat history */}
          <button
            onClick={toggleChatVisibility}
            className="px-4 py-2 text-sm bg-gray-200 hover:bg-gray-300 rounded-lg font-medium transition-colors"
          >
            {isChatVisible ? "Hide Full Transcript" : "Show Full Transcript"}
          </button>

          {/* Pipeline Button (for final IELTS evaluation) */}
          <button
            onClick={() => setAppState("finished")}
            disabled={fullChatHistory.length === 0 || appState !== 'idle'} // Only enabled if conversation started and app is idle
            className="px-4 py-2 text-sm bg-blue-500 hover:bg-blue-600 text-white rounded-lg font-medium disabled:opacity-50 transition-colors"
          >
            End Interview & Get Feedback
          </button>

          {/* NEW Reset Button */}
          <button
            onClick={handleResetSession}
            disabled={['transcribing', 'asking', 'speaking', 'playing', 'summarizing', 'processing_feedback'].includes(appState)}
            className="px-4 py-2 text-sm bg-red-500 hover:bg-red-600 text-white rounded-lg font-medium disabled:opacity-50 transition-colors"
          >
            Reset Session
          </button>
        </div>

        {/* Conditionally rendered Full Chat History */}
        {isChatVisible && (
          <div className="flex-grow border rounded-lg p-4 h-96 overflow-y-auto space-y-3 bg-white shadow-inner mb-4">
            <h2 className="text-lg font-bold text-gray-700 mb-2 border-b pb-1">Conversation Transcript</h2>
            {fullChatHistory.length === 0 && <p className="text-center text-gray-500 italic">No conversation yet. Start the interview!</p>}
            {fullChatHistory.map((msg, index) => (
              <div key={index} className={`p-3 rounded-lg ${msg.role === 'user' ? 'bg-blue-50 text-blue-800 text-right ml-auto' : 'bg-gray-50 text-gray-800 text-left mr-auto'} max-w-[85%] shadow-sm`}>
                <span className="font-bold text-sm">{msg.role === 'user' ? 'You' : 'Examiner'}</span>: <span className="text-base">{msg.content}</span>
              </div>
            ))}
            {/* Real-time status indicators within chat history */}
            {appState === 'transcribing' && <p className="text-center text-gray-500 italic text-sm py-2">Transcribing your response...</p>}
            {appState === 'asking' && <p className="text-center text-gray-500 italic text-sm py-2">Examiner is thinking of the next question...</p>}
            {appState === 'speaking' && <p className="text-center text-gray-500 italic text-sm py-2">Examiner is preparing to speak...</p>}
            {appState === 'summarizing' && <p className="text-center text-blue-500 italic text-sm py-2">Summarizing previous turns for context...</p>}
          </div>
        )}

        {/* Microphone Button - Primary interaction */}
        <button
          onClick={handleMicPress}
          disabled={!isAudioApiSupported || ['transcribing', 'asking', 'speaking', 'playing', 'summarizing', 'processing_feedback'].includes(appState)}
          className={`w-full max-w-xs mx-auto h-14 px-6 py-3 rounded-full font-bold text-lg text-white disabled:opacity-50 flex items-center justify-center gap-3 transition-all duration-300 shadow-lg
            ${appState === 'recording' ? 'bg-red-600 hover:bg-red-700' :
              (appState === 'idle' && fullChatHistory.length === 0) ? 'bg-indigo-600 hover:bg-indigo-700' :
              (appState === 'idle' && fullChatHistory.length > 0) ? 'bg-green-600 hover:bg-green-700' :
              'bg-gray-400 cursor-not-allowed'
            }`}
        >
          <Mic className="h-6 w-6" /> {/* Mic icon */}
          <span className="whitespace-nowrap">
            {appState === 'recording' ? 'Stop Recording' :
             (appState === 'idle' && fullChatHistory.length === 0) ? 'Start Interview' :
             (appState === 'idle' && fullChatHistory.length > 0) ? 'Record Answer' :
             (appState === 'show_results' || appState === 'error') ? 'Start New Interview' :
             'Processing...'}
          </span>
        </button>

        {/* Audio Player (Hidden but controls examiner's voice) */}
        <audio ref={audioPlayerRef} controls className="hidden"></audio>

        {/* Results Panel (for IELTS feedback) */}
        {appState === 'show_results' && (
          <div className="border rounded-lg p-6 bg-white shadow-lg mt-6">
            <h2 className="text-2xl font-bold mb-4 text-gray-800 text-center">Your Interview Feedback</h2>
            {feedbackResults.length > 0 ? (
              feedbackResults.map((res, index) => (
                <div key={index} className="mb-5 p-4 bg-gray-50 rounded-md border border-gray-200">
                  <h3 className="font-extrabold text-lg mb-2 text-blue-700">{res.criterion}</h3>
                  {/* Dangerously set HTML for Markdown conversion */}
                  <div className="prose prose-sm max-w-none" dangerouslySetInnerHTML={{ __html: markdownToHtml(res.feedback) }} />
                </div>
              ))
            ) : (
              <p className="text-center text-gray-600 italic">No feedback available. Something might have gone wrong with the pipeline.</p>
            )}
            <button
              onClick={handleMicPress} // This button restarts the interview correctly
              className="mt-6 w-full px-6 py-3 bg-green-600 text-white rounded-full font-bold text-lg hover:bg-green-700 transition-colors shadow-md"
            >
              Start New Interview
            </button>
          </div>
        )}
        </>
        )}
      </div>
    </>
  );
}

/**
 * Basic utility function to convert simple Markdown (bold, italic, new lines) to HTML.
 * This is used for rendering feedback from the LLM.
 * @param md The markdown string.
 * @returns HTML string.
 */
function markdownToHtml(md: string) {
  return md
    .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>') // bold
    .replace(/\*(.*?)\*/g, '<em>$1</em>') // italic
    .replace(/\n/g, '<br />'); // line breaks
}
